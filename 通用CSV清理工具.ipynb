{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f4ddc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始執行清理流程...\n",
      "==================================================\n",
      "\n",
      "開始處理：北大武_活動資料_使用者ID版.csv\n",
      "----------------------------------------\n",
      "原始資料筆數：905\n",
      "重複資料筆數：241\n",
      "\n",
      "重複資料範例（前3筆）：\n",
      "                  ID              日期   總移動距離      總時間 總爬升高度 總下降高度\n",
      "yuhjaanlin1700262305 2025/07/21・1 日・  3.9 km  1 h 3 m 247 m 234 m\n",
      "           a29751765 2025/07/20・1 日・ 13.9 km 6 h 40 m 653 m 706 m\n",
      "              k32100 2025/07/20・1 日・   14 km 8 h 31 m 926 m 813 m\n",
      "\n",
      "清理完成！\n",
      "清理後資料筆數：664\n",
      "已移除 241 筆重複資料\n",
      "清理比例：26.6%\n",
      "\n",
      " 檔案 北大武_活動資料_使用者ID版.csv 處理完成！\n",
      "==================================================\n",
      "驗證清理結果...\n",
      "------------------------------\n",
      "北大武_活動資料_使用者ID版.csv 清理成功！無剩餘重複資料\n",
      "驗證完成！\n"
     ]
    }
   ],
   "source": [
    "from pickle import FALSE\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "# 指定要清理的CSV檔案路徑\n",
    "TARGET_FILE = \"北大武_活動資料_使用者ID版.csv\" # 只清理單一檔案\n",
    "\n",
    "# FOLDER_PATH = \"./白姑大山/book/網頁資料\"  # 注意!!! 這裡是清理資料夾里全部的CSV檔案\n",
    "\n",
    "# 是否要處理資料夾內的所有CSV檔案\n",
    "PROCESS_ALL_FILES = False  # 要使用就調成True\n",
    "\n",
    "def analyze_duplicates(file_path):\n",
    "    \"\"\"分析CSV檔案的重複資料\"\"\"\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"找不到檔案：{file_path}\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    original_count = len(df)\n",
    "    duplicates = df.duplicated()\n",
    "    duplicate_count = duplicates.sum()\n",
    "    \n",
    "    return {\n",
    "        'file_path': file_path,\n",
    "        'original_count': original_count,\n",
    "        'duplicate_count': duplicate_count,\n",
    "        'df': df,\n",
    "        'duplicates': duplicates\n",
    "    }\n",
    "\n",
    "def clean_duplicates(file_path):\n",
    "    \"\"\"清理CSV檔案中的重複資料\"\"\"\n",
    "    \n",
    "    print(f\"\\n開始處理：{file_path}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 分析重複資料\n",
    "    analysis = analyze_duplicates(file_path)\n",
    "    if analysis is None:\n",
    "        return False\n",
    "    \n",
    "    original_count = analysis['original_count']\n",
    "    duplicate_count = analysis['duplicate_count']\n",
    "    df = analysis['df']\n",
    "    \n",
    "    print(f\"原始資料筆數：{original_count}\")\n",
    "    print(f\"重複資料筆數：{duplicate_count}\")\n",
    "    \n",
    "    if duplicate_count == 0:\n",
    "        print(\"沒有重複資料，無需清理\")\n",
    "        return True\n",
    "    \n",
    "    # 顯示重複資料範例\n",
    "    duplicate_rows = df[analysis['duplicates']]\n",
    "    print(f\"\\n重複資料範例（前3筆）：\")\n",
    "    print(duplicate_rows.head(3).to_string(index=False))\n",
    "    \n",
    "    # 移除重複資料\n",
    "    df_cleaned = df.drop_duplicates()\n",
    "    cleaned_count = len(df_cleaned)\n",
    "    \n",
    "    # 儲存清理後的資料\n",
    "    df_cleaned.to_csv(file_path, index=False)\n",
    "    \n",
    "    print(f\"\\n清理完成！\")\n",
    "    print(f\"清理後資料筆數：{cleaned_count}\")\n",
    "    print(f\"已移除 {duplicate_count} 筆重複資料\")\n",
    "    print(f\"清理比例：{duplicate_count/original_count*100:.1f}%\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# 執行清理流程\n",
    "print(\"開始執行清理流程...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if PROCESS_ALL_FILES:\n",
    "    # 處理資料夾內的所有CSV檔案\n",
    "    csv_files = glob.glob(os.path.join(FOLDER_PATH, \"*.csv\"))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"在 {FOLDER_PATH} 中找不到CSV檔案\")\n",
    "    else:\n",
    "        print(f\"發現 {len(csv_files)} 個CSV檔案：\")\n",
    "        for file in csv_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "        \n",
    "        total_cleaned = 0\n",
    "        for file in csv_files:\n",
    "            if clean_duplicates(file):\n",
    "                total_cleaned += 1\n",
    "        \n",
    "        print(f\"\\n批量清理完成！共處理 {len(csv_files)} 個檔案\")\n",
    "else:\n",
    "    # 處理單一檔案\n",
    "    if clean_duplicates(TARGET_FILE):\n",
    "        print(f\"\\n 檔案 {TARGET_FILE} 處理完成！\")\n",
    "    else:\n",
    "        print(f\"\\n檔案 {TARGET_FILE} 處理失敗！\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 驗證清理結果\n",
    "print(\"驗證清理結果...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if PROCESS_ALL_FILES:\n",
    "    csv_files = glob.glob(os.path.join(FOLDER_PATH, \"*.csv\"))\n",
    "    for file in csv_files:\n",
    "        analysis = analyze_duplicates(file)\n",
    "        if analysis:\n",
    "            remaining_duplicates = analysis['duplicate_count']\n",
    "            print(f\"{file}: {remaining_duplicates} 筆重複資料\")\n",
    "else:\n",
    "    analysis = analyze_duplicates(TARGET_FILE)\n",
    "    if analysis:\n",
    "        remaining_duplicates = analysis['duplicate_count']\n",
    "        if remaining_duplicates == 0:\n",
    "            print(f\"{TARGET_FILE} 清理成功！無剩餘重複資料\")\n",
    "        else:\n",
    "            print(f\"{TARGET_FILE} 仍有 {remaining_duplicates} 筆重複資料\")\n",
    "\n",
    "print(\"驗證完成！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
